---
title: How to Measure the Impact of Research Data
author:
-   Alex Ball (DCC)
-   Monica Duke (DCC)
date: 'DRAFT: \today'
---

> This guide will help you to measure the impact of research data,
> whether your own or that of your department/institution.
> It provides ...
> This guide should interest researchers and principal investigators working on data-led research
> and administrators working with research quality assessment submissions.

Why measure the impact of research data?
========================================

A key measure of the worth of research is the impact it has,
both within the academic community and beyond.
In recent years funding bodies have placed increasing emphasis on monitoring the potential and actual impact of the research projects they fund.

Since 1997, the NSF has judged the merit of research proposals on their intellectual merit and their broader impact [@nsb2011mrc].
In the UK, impact plans became part of the bidding process for all Research Councils in 2009,
though in 2010 the purpose of the plans was clarified by reformulating them as Pathways to Impact [@hodgson.porter2010pti].
In this part of their proposals,
researchers are asked to consider how they might maximise the academic, societal and economic impact of their research.

At the other end of the research lifecycle,
the 2014 Research Excellence Framework in the UK will include impact as an explicit element alongside outputs and environment [@ref2011dar].

(Explanation of what impact is, and why it is the *mot du jour* amongst policy makers.)
The measurement of impact goes beyond evaluating the intrinsic academic quality and value of research as judged by other 
academics, to consider how wider societal needs have been met. 
for example influence on practice or policy, addressing pressing (societal) questions or problems, 
generating wealth, driving industrial innovations, or taking on board community priorities when planning research (such as using 
patient views as input to design medical studies).
  
In 2014-2015 the Higher Education Funding Council For England (HEFCE) was undertaking a review of the role of metrics in research 
assessment, to consider how metrics can be used to assess the quality and broader impact of scientific and scholarly 
research [@hefce2014irm].   In October 2014, the EPSRC set out its clarification of expectations [@epsrc2014cla], explaining that research
organisations are expected to record requests to access data that they hold; requests are to be logged, and will help build 
evidence of impact (Expectation III). Furthermore, although this funding body does not extend the expectation of logging access
to data held by external repositories, organisations are reminded that such information would be "a valuable indicator of impact".

EPSRC explains that information on data access informs decisions about data retention; the funding body
does not expect data to be retained if it has attracted no interest, once a period of 10 years has lapsed from 
last access (Expectation VII). Data citations or "any other metric based on reliable source of evidence widelt accepted at the time"
can be used as evidence of interest shown in the dataset [@epsrc2014cla].

For researchers in short term: getting credit for full range of research outputs.
Link to REF 2020 (and any other research QA schemes internationally that do/might count datasets)?
By monitoring usage, get to know which forms of data preparation and data publication work the best.

May lead to collaboration with peers who are using the data,
or to knowledge of communities who were not the original intended audience (e.g. the public).

Longer term:
increased acceptance of data as research output will drive up data sharing,
improving the quality of science and opening up opportunities for more forms of research.


Impact measurement concepts
===========================

Relationship between the impact of resources and impact of researchers.

Reminder of how bibliometrics and scientometrics work for traditional publications. Citations, impact factor.

The Journal Impact Factor (JIF) is a measure applied to individual journals which tries to compensate for 
variations in journal size.  The JIF is derived from the total citations for a journal made in a specific year, to 
articles published in the previous two years, divided by the number of articles. (Certain items which are
not considered to be substantive articles, e.g. news items or letters, are not included).
It is published by the *Journal Citation Reports (JCR)* (mention Thomson Scientific)
Although the JIF is a measure of the impact of a whole journal not individual articles, it is often used as a proxy
for the prestige of the journal and impact of the articles and authors published in that journal.

Microattribution, ?nanopublication. 

Introduction to measures of impact other than citations: page views, download rates, tweets and retweets, trackbacks (from blogs, etc.), tagging, social bookmarking.

(intro to say these are types of online behaviour?)

Page views indicate the level of interest that has been attracted by an online page that gives access to a research item.
The page is often one which displays information such as author and title, short description, and a link to allow
download of the item.  Page views indicate that the item has drawn a level of attention sufficient for users to want to
find out more information about it and guarantees a knowledge of existence. (Although some page views will be for HTML article?)

Download rates are based on usage log data and reflect the number of times that a whole item has been downloaded from a link.  The assumption
is that the item has been judged to be of sufficient interest to warrant a closer look (indicating a judgement of value
deeper than simply 'looks interesting') but there is no guarantee that a download does lead to use. Once downloaded the item may simply
not be looked at again, or found not to be of value.

Twitter is a social networking tool that enables users to send short messages (limited to 140 chracters) to their followers. Tweets can provide pointers to research by mentioning that research or sharing links directly to research or to pages discussing it or providing access.
Tweets can be used as a measure of interest generated in the item, by counting the number of references, or tweets, that are about that 
item.  The content of the tweet however has to be examined to find out if mentions are positive, neutral or negative. (a screenshot
example of a recommendation perhaps?) Tweets can be forwarded, called a retweet, showing a passing on of the recommendation.  

What about news articles, comments, social commentary

Impact in different communities - e.g. researcher use, public engagement, policy and practice

Impact measurement services
===========================

Thomson Reuters Data Citation Index
------------------------------------

In October 2012, Thomson Reuters launched the Data Citation Index (DCI) as part of its Web of Knowledge service [@herther2012trt].
It provides records at four levels of granularity:
nanopublication,
dataset,
data study (a research activity producing one or more data sets) and
repository.
The records can be searched and filtered in various ways, in the same way as (and indeed in combination with) the other indices in Web of Knowledge.
The records are linked so that, for example, from a repository record one can view records for the data studies and data sets held by that repository.
Sample citations are also provided.

On each record, the DCI displays the number of times the entity has been cited in Web of Knowledge.
Recognising the variety of ways in which data sets and repositories can be cited,
the DCI counts not only entries in the reference list but also less formal citations that occur elsewhere in scholarly papers
(for example, in the abstract or acknowledgements).

Selection for inclusion in the DCI is at the level of whole repositories rather than individual data sets or studies.
The criteria used for selecting repositories include
longevity,
sustainability,
activity (in terms of new data being deposited),
metadata held for the data (ideally in English, with links to associated literature, funding information, and so on), and
quality assurance procedures [@tr2012res].

ImpactStory
-----------
This service allows researchers to build a profile to showcase various contributions and related activity. After
registration (which in 2014 costs $60/year) different products including data, software, presentations as well as articles
can be added to the researcher's profile. Sources used include vimeo and youtube for videos,
github for software and presentations on Slideshare. Identifiers such as PubMedIds, DOIs and URLS can be
registered so that the items can be tracked.
(maybe need to mention Figshare?)

ImpactStory tracks metrics generated by external services and updates the profile with products
and metrics.  Email notifications, sent weekly as a default,
are used to highlight the performance of research items in the profile, such as achieving views on external
services or acquiring new readers.  Typical metrics are Twitter tweets, views, saves, downloads,posts from blogs
and Facebook, bookmarks on citeulike and delicious. A feedbook forum is used to
obtain feedback from users, with a voting system to promote popular ideas to the top of the heap for development.

Impact Story operates as a non-profit registered in the USA. It
was known as total-impact until 2012 and has received funding from the Open Knowledge Foundation, the National
Science Foundation, JISC and the Sloan Foundation.
The data collected by the service is made open (unless restricted by
third parties), and the code and governance are also open. An export feature is available to export the profile.

ReaderMeter
-----------
Another service that is currently suspended, ReaderMeter tries to mirror the H-Index and G-Index by analysing 
data about readership (based on bookmarks) from the Mendeley service. Further information is available from the 
DCC [@dcc2013rea].

ScienceCard
-----------
ScienceCard is a service which is currently suspended.  It started out as a project from a hackaton.  The aim was to create
a reasercher-centred service to import scientific articles into a profile. Users would be able to add their articles through
a DOI, PubMedID or via their ORCID account.  Article-level metrics would then be computed around the researcher.  It is possible that
the service may be relaunched in the future.

Currently offline, promises it will be back.

PLoS Article-Level Metrics
--------------------------

In 2009, the Public Library of Science (PLoS) launched its Article-Level Metrics (ALM) service.^[PLOS ALM website: <http://article-level-metrics.plos.org/alm-info/>]
This compiles a set of impact indicators from PLoS's own systems and various other services,
and makes them available in both a visual way and via an application programming interface (API).

The metrics compiled include

  * usage statistics (views and downloads) from PLoS and Pubed Central;
  * interactions (comments, notes, ratings) on the PLoS website;
  * citations identified by Scopus, Web of Science and others;
  * references made in social networks like Twitter and Facebook, on various blogging platforms, or on Wikipedia.

The metrics are displayed on the landing pages for PLoS articles, and can also be compiled into custom reports.^[ALM Reports website: <http://almreports.plos.org//>]

PLoS released the source code for the ALM application in 2011.^[Lagotto (Article-Level Metrics) source code repository: <https://github.com/mfenner/lagotto>]
It was used as the basis of the ScienceCard service, which provided an author-centric view on the same data [@fenner2011asc].
It was also taken up by other publishers and service providers, most significantly by CrossRef Labs,
meaning statistics are available for many non-PLoS papers as well [@lin.fenner2014osc].

While the implementations of the software so far have concentrated on papers, the software itself is resource-type agnostic, so could be applied to datasets.

PloS Impact Explorer
--------------------

'This page mashes up alt-metrics data from Altmetric with articles from the Public Library of Science (PLoS).
Check which articles are seeing the most buzz from social media sites, newspapers and in online reference managers.'

available for PLOS articles, measures 'reach', API available
started in 2009 [ref]

PaperCritic
-----------
PaperCritic offers a platform for open reviewing of scientific work.  It uses the Mendeley API to allow users to add rating and reviewing
features to their Mendeley collection. The aim is to encourage post-publication discussion in a more social environment.
(It is not clear if PaperCritic is actively being developed - last blog posts from around 2012
suggesting community templates for review structuring.)

Crowdometer
-----------

ResearchGate Score
------------------
creates and RG score

Google Scholar
--------------

Microsoft Academic Search
-------------------------

Plum Analytics
---------------
Acquired by EBSCO at the start of 2014 [@harris2014acq], Plum Analytics was founded in 2012 and developed its product
with the University of Pittsburgh as a partner. Plum X, its main product, aims to illustrate a more comprehensive
picture of research impact beyond (but including) citations. Amongst its customers it counts universities, 
corporations, publishers and funders, and reports rapid growth since the acquisition.

PlumX aggregates information about the use of research outputs, including "non-traditional"
types, from external sources. It displays the information in various ways and provides different methods of interacting with the
aggregated output.  The information sources that are aggregated include Dryad for information about datasets, Amazon for 
books and YouTube for videos.  It aims to highlight impact from the previous eighteen months, to fill the gap created by 
the publication lag, alongside citation data.

The metrics gathered by Plum X are grouped into five categories: usage (clicks, downloads, views), captures (bookmarks, favourites),
mentions (comments, reviews), social media (likes, shares, tweets) and citations. These categories are then displayed in dashboard views
that can be controlled to present the data in different ways.  For example a university can view data by level,
(e.g. school and programme), and drill down through the levels to individual researcher information, or to individual artefact (e.g. a 
specific article).

A 'plum print' summary is available for embedding into other sites, such as the institutional repository.  The information shown in the 
embedded widget is customisable and links to the original source are made available. Researchers can help seed the information available 
by making the link with their profiles in other systems (e.g. by entering a Slideshare ID).  An example of aggregated information
is that for books, which takes in Amazon, GoodReads and Wikipedia presence [@michalek2014plu].

Altmetric
----------
article-centered, subscription-based, web API for developers, doughnut-shaped badge

SocialCite
-----------
reasons behind citations - new company

Tips for raising research data impact
=====================================

(Need to search literature for evidence, examples and case studies.)
(possible examples of how altmetrics help monitor use perhaps in Cameron PLOS Opens Blog post 
Altmetrics: What are they good for?)

Provide citation/discovery metadata. Use of identifers.

Provide enough metadata for reuse. Reproducibility.

Open access.

Deposit in major repository. (Too early to discuss certified repositories?)

Highlighting resources using social media. Academia.edu, ResearchGate, Mendeley, etc.

include identifiers in tweets and blogs to improve accuracy of counting by service tools (recommended by [@featherstone2014stm] and [@niso2014ami])


Current issues and challenges
=============================

Immaturity of the area.

Due to the early stage of development of some of the tools (specially for altmetrics) and a lack of standardisation in
what they are measuring, results may not be comparable, which limits their use in the processes for determining rewards
based on scholarly impact [@featherstone2014stm].  The study of the reliability and validity of the data is a work in progress ; "there is 
a tendency to include what can be counted, rather than to include what adds particular value" [@niso2014ami].
(check data metrics workshop report)

Relationship between alt metrics and traditional citation-based metrics: measuring different things.

Possibilities (or otherwise) for gaming the system. Ethical behaviour.

Gaming of metrics can be described as "behaviour that is meant to unfairly manipulate those metrics, generally for one's
benefit." [@niso2014ami] 
self-citing and self-tweeting not considered unethical in themselves, but need to be factored into calculations [@featherstone2014stm]
 and consensus is still emerging on where the line lies between acceptable promotion and cheating [@niso2014ami].

Wider acceptance of alt metrics.
(acceptance by whom? Funders? Institutions? Academics?)


At least *some* academics have reservations about the culture of metrics and the use of measures
to enforce competition. Audits, monitoring and data collection are considered to be a force that shapes academia, 
in a manner that is too dominating.  Scepticism is expressed that value can be captured through metrics, particularly for research outputs. 
The influence of metrics on decision-making e.g. choosing to publish in a journal with a high impact factor, is questioned. 
In this light, the inclusion of data metrics is seen as an example of the increasing obsession with measurement, 
and is not necessarily viewed as a positive development [@burrows2013lwn].  

David Colquhoun, in his submission to HEFCE on the role of metrics in research assessment, cautions against conflating certain metrics 
with quality.  In particular he singles out some altmetrics such as Tweets for criticism, suggesting that popularity of retweeting may
reflect trendiness, rather than quality.  In the examples given, he suspects that the content of the title determines what becomes
popular, and not the article itself, which may not even have been read.  This limits the impact that the article has had on the 
general public, let alone other academics [@colquhoun2014sub]. He expresses concern that the glamourising of articles leads to 
inaccuracies, and to overlooking of the detail of the article and its quality. He strongly recommends against the use of altmetrics [@colquhoun2014wos].

One other aspect of acceptance is trust in the data quality [@niso2014ami] (which still requires work as mentioned previously).

Even proponents of altmetrics recognise that current practice for career progression focuses on pubslishing traditional
journal articles, and that change is required for funders to start recognising other scholarly works and impact [@konkiel.piwowar.priem2014tio].

The effects of time

Number of citations tend to increase over time. Sometimes there is a publication lag (which altmetrics can fill).  
However altmetrics will not exist for earlier contributions (pre-Twitter, Facebook etc).

Contributor roles

Identifying who contributed what to the data - generating, cleaning, documenting, curating
(Is this more about credit than impact?)
[@niso2014ami] suggests that alterntive assessment metrics "could play a role in this area by tracking or measuring credit for
more non-traditional research outputs such as research data"

Acknowledgements
================

Any acknowledgements will go here.


Further information
===================

\setlength{\parindent}{0pt}\nonzeroparskip\color{dccblue}\small
A list of related DCC resources goes here.

\normalcolor
A list of non-DCC resources and publications goes here.


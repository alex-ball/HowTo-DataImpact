% How to Measure the Impact of Research Data
% Alex Ball; Monica Duke
% DRAFT: \today

> This guide will help you to measure the impact of research data, whether your own or that of your department/institution. It provides ... This guide should interest researchers and principal investigators working on data-led research and administrators working with research quality assessment submissions.

Why measure the impact of research data?
========================================

A key measure of the worth of research is the impact it has, both within the academic community and beyond. In recent years funding bodies have placed increasing emphasis on monitoring the potential and actual impact of the research projects they fund.

Since 1997, the NSF has judged the merit of research proposals on their intellectual merit and their broader impact [@nsb2011mrc]. In the UK, impact plans became part of the bidding process for all Research Councils in 2009, though in 2010 the purpose of the plans was clarified by reformulating them as Pathways to Impact [@hodgson.porter2010pti]. In this part of their proposals, researchers are asked to consider how they might maximise the academic, societal and economic impact of their research.

At the other end of the research lifecycle, the 2014 Research Excellence Framework in the UK will include impact as an explicit element alongside outputs and environment [@ref2011dar].

(Explanation of what impact is, and why it is the *mot du jour* amongst policy makers.)
The measurement of impact goes beyond evaluating the intrinsic academic quality and value of research as judged by other 
academics, to consider how wider societal needs have been met. 
for example influence on practice or policy, addressing pressing (societal) questions or problems, 
generating wealth, driving industrial innovations, or taking on board community priorities when planning research (such as including 
patient input in the design of medical studies).
  
In 2014-2015 the Higher Education Funding Council For England (HEFCE) was undertaking a review of the role of metrics in research 
assessment, to consider how metrics can be used to assess the quality and broader impact of scientific and scholarly 
research [@hefce2014irm].  

For researchers in short term: getting credit for full range of research outputs. Link to REF 2020 (and any other research QA schemes internationally that do/might count datasets)? By monitoring usage, get to know which forms of data preparation and data publication work the best.
May lead to collaboration with peers who are using the data, or to knowledge of communities who were not the original intended audience
(e.g. the public).

Longer term: increased acceptance of data as research output will drive up data sharing, improving the quality of science and opening up opportunities for more forms of research.


Impact measurement concepts
===========================

Relationship between the impact of resources and impact of researchers.

Reminder of how bibliometrics and scientometrics work for traditional publications. Citations, impact factor.

The Journal Impact Factor (JIF) is a measure applied to individual journals which tries to compensate for 
variations in journal size.  The JIF is derived from the total citations for a journal made in a specific year, to 
articles published in the previous two years, divided by the number of articles. (Certain items which are
not considered to be substantive articles, e.g. news items or letters, are not included).
It is published by the *Journal Citation Reports (JCR)* (mention Thomson Scientific)
Although the JIF is a measure of the impact of a whole journal not individual articles, it is often used as a proxy
for the prestige of the journal and impact of the articles and authors published in that journal.

Microattribution, ?nanopublication. 

Introduction to measures of impact other than citations: page views, download rates, tweets and retweets, trackbacks (from blogs, etc.), tagging, social bookmarking.

(intro to say these are types of online behaviour?)

Page views indicate the level of interest that has been attracted by an online page that gives access to a research item.
The page is often one which displays information such as author and title, short description, and a link to allow
download of the item.  Page views indicate that the item has drawn a level of attention sufficient for users to want to
find out more information about it and guarantees a knowledge of existence. (Although some page views will be for HTML article?)

Download rates are based on usage log data and reflect the number of times that a whole item has been downloaded from a link.  The assumption
is that the item has been judged to be of sufficient interest to warrant a closer look (indicating a judgement of value
deeper than simply 'looks interesting') but there is no guarantee that a download does lead to use. Once downloaded the item may simply
not be looked at again or used, or found not to be of value.

Impact in different communities - e.g. researcher use, public engagement, policy and practice

Impact measurement services
===========================

Thomson Reuters Data Citation Index
------------------------------------

In October 2012, Thomson Reuters launched the Data Citation Index (DCI) as part of its Web of Knowledge service [@herther2012trt]. It provides records at four levels of granularity: nanopublication, dataset, data study (a research activity producing one or more data sets) and repository. The records can be searched and filtered in various ways, in the same way as (and indeed in combination with) the other indices in Web of Knowledge. The records are linked so that, for example, from a repository record one can view records for the data studies and data sets held by that repository. Sample citations are also provided.

On each record, the DCI displays the number of times the entity has been cited in Web of Knowledge. Recognising the variety of ways in which data sets and repositories can be cited, the DCI counts not only entires in the reference list but also less formal citations that occur elsewhere in scholarly papers (for example, in the abstract or acknowledgements).

Selection for inclusion in the DCI is at the level of whole repositories rather than individual data sets or studies. The criteria used for selecting repositories include longevity, sustainability, activity (in terms of new data being deposited), metadata held for the data (ideally in English, with links to associated literature, funding information, and so on), and quality assurance procedures [@tr2012res].

ImpactStory
-----------

ReaderMeter
-----------

ScienceCard
-----------

PloS Impact Explorer
--------------------

PaperCritic
-----------

Crowdometer
-----------

ResearchGate Score
------------------

Google Scholar
--------------

Microsoft Academic Search
-------------------------

Plum Analytics
---------------
Acquired by EBSCO at the start of 2014 [@harris2014acq], Plum Analytics was founded in 2012 and developed its product
with the University of Pittsburgh as a partner. Plum X, its main product, aims to illustrate a more comprehensive
picture of research impact beyond (but including) citations. Amongst its customers it counts universities, 
corporations, publishers and funders, and reports rapid growth since the acquisition.

PlumX aggregates information about the use of research outputs, including "non-traditional"
types, from external sources. It displays the information in various ways and provides different methods of interacting with the
aggregated output.  The information sources that are aggregated include Dryad for information about datasets, Amazon for 
books and YouTube for videos.  It aims to highlight impact from the previous eighteen months, to fill the gap created by 
the publication lag, alongside citation data.

The metrics gathered by Plum X are grouped into five categories: usage (clicks, downloads, views), captures (bookmarks, favourites),
mentions (comments, reviews), social media (likes, shares, tweets) and citations. These categories are then displayed in dashboard views
that can be controlled to present the data in different ways.  For example a university can view data by level,
(e.g. school and programme), and drill down through the levels to individual researcher information, or to individual artefact (e.g. a 
specific article).

A 'plum print' summary is available for embedding into other sites, such as the institutional repository.  The information shown in the 
embedded widget is customisable and links to the original source are made available. Researchers can help seed the information available 
by making the link with their profiles in other systems (e.g. by entering a Slideshare ID).  An example of aggregated information
is that for books, which takes in Amazon, GoodReads and Wikipedia presence [@michalek2014plu].

Tips for raising research data impact
=====================================

(Need to search literature for evidence, examples and case studies.)
(possible examples of how altmetrics help monitor use perhaps in Cameron PLOS Opens Blog post 
Altmetrics: What are they good for?)

Provide citation/discovery metadata. Use of identifers.

Provide enough metadata for reuse. Reproducibility.

Open access.

Deposit in major repository. (Too early to discuss certified repositories?)

Highlighting resources using social media. Academia.edu, ResearchGate, Mendeley, etc.


Current issues and challenges
=============================

Immaturity of the area.

Relationship between alt metrics and traditional citation-based metrics: measuring different things.

Possibilities (or otherwise) for gaming the system. Ethical behaviour.

Wider acceptance of alt metrics.
(acceptance by whom? Funders? Institutions? Academics?)


At least *some* academics have reservations about the culture of metrics and the use of measures
to enforce competition. Audits, monitoring and data collection are considered to be a force that shapes academia, 
in a manner that is too dominating.  Scepticism is expressed that value can be captured through metrics, particularly for research outputs. 
The influence of metrics on decision-making e.g. choosing to publish in a journal with a high impact factor, is questioned. 
In this light, the inclusion of data metrics is seen as an example of the increasing obsession with measurement, 
and is not necessarily viewed as a positive development [@burrows2013lwn].  

David Colquhoun, in his submission to HEFCE on the role of metrics in research assessment, cautions against conflating certain metrics 
with quality.  In particular he singles out some altmetrics such as Tweets for criticism, suggesting that popularity of retweeting may
reflect trendiness, rather than quality.  In the examples given, he suspects that the content of the title determines what becomes
popular, and not the article itself, which may not even have been read.  This limits the impact that the article has had on the 
general public, let alone other academics [@colquhoun2014sub]. He expresses concern that the glamourising of articles leads to 
inaccuracies, and to overlooking of the detail of the article and its quality. He strongly recommends against the use of altmetrics [@colquhoun2014wos].
